{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab2ed237",
   "metadata": {},
   "source": [
    "# Токенизация\n",
    "\n",
    "В этом ноутбуке мы поговорим о том, что такое токенайзер, как его создать и какие интересные аспекты поведения моделей связаны с токенизацией.\n",
    "Чтобы посмотреть, как работают разные токенизаторы, можно поиграть с этим [сайтом](https://tiktokenizer.vercel.app/).\n",
    "\n",
    "Зачем вообще нужна токенизация? Это способ представить текст (или картинку) в виде достаточно небольших кусочков, которые можно передать как некоторые дискретные единицы в нейронную сеть или другую модель. В идеале, токенизатор должен разбивать текст так, чтобы каждый токен нес в себе больше информации, чем отдельный символ, но словарь возможных токенов был достаточно универсальным.\n",
    "\n",
    "Существует множество разных токенизаторов, самый простой - просто разбить текст на отдельные символы.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b69030e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Hi! Please help me to write 你好 in english!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981133f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Tokenized sequence: \", \" \".join([str(ord(x)) for x in text]))\n",
    "print(f\"Number of letters: \", len([ord(x) for x in text]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606d5bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = text.encode(\"utf-8\") # todo: try utf-16,utf-32\n",
    "print(\"Tokenized sequence (raw): \", tokens)\n",
    "# Выведите численные представления\n",
    "print(\"Tokenized sequence: \", tokens)\n",
    "print(f\"Number of tokens: \", len(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0948e5b3",
   "metadata": {},
   "source": [
    "Вопросы: \n",
    "- если мы будем использовать кодировку в utf-8, какого размера будет словарь? \n",
    "- В чем минус такой кодировки?\n",
    "- какие еще способы токенизации вы знаете?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077279de",
   "metadata": {},
   "source": [
    "\n",
    "Вообще, токенизаторы не являются частью модели, но модель использует токены и никогда не видит сам текст:\n",
    "\n",
    "![alt_text](../images/tokenizer.png)\n",
    "\n",
    "Вопрос: что будет, если заменить токенищатор модели после  ее обучения?\n",
    "Так как токенизатор независимая единица, он использует свой собственный обучающий датасет (который может отличаться от того, на котором претрейнилась модель). \n",
    "\n",
    "Вопросы: \n",
    "- Почему модели долго не умели считать буквы в словах?\n",
    "- Почему моделям часто сложно делать простейшие арифметические операции?\n",
    "- Почему \"ранние\" модели плозо говорили на русском или китайском?\n",
    "- Почему YAML лучше JSON для LLM?\n",
    "- Почему модель иногда вдруг начинает генерировать бред?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623f2550",
   "metadata": {},
   "source": [
    "##  BPE алгоритм\n",
    "\n",
    "Byte-Pair Encoding (BPE) это (по идее авторов) метод сжатия текстовых данных. Основная идея BPE заключается в итеративном объединении наиболее часто встречающихся пар символов (в текстовом корпусе) и формировании из них токенов, которые в последующем используются для токенизации.\n",
    "\n",
    "### Алгоритм\n",
    "1) Подготавливается словарь токенов (пока пустой)\n",
    "2) Весь корпус разбивается на символы (байты). Уникальные символы добавляются в словарь токенов. Вопрос: почему их наджо сразу добавить в словарь?\n",
    "3) В цикле (пока не будет достигнут желаемый размер словаря):\n",
    "   1) Рассчитывается частот каждой соседней пары токенов в корпусе\n",
    "   2) Наиболее частая пара объединяется в токен: добавляется в словарь, после чего объединяется в текстовом корпусе.\n",
    "\n",
    "Базовый алгоритм обычно дополняется разными важными для разработчиков вещами:\n",
    "- Появляется претокенизация (с помощью регулярок) для подготовки корпуса - он разбивается на чанки, что ускоряет BPE.\n",
    "- В GPT-2 нельзя объединять в пару токены разных типов (знак препинания и букву, например).\n",
    "- D GPT-3 нельзя объединять более 3 цифр подряд.\n",
    "- В LLaMA нельзя создать токены только из пробелов или управляющих символов.\n",
    "\n",
    "Кроме того, эффективность кодировки сильно зависит от корпуса, поэтому токенизаторы довольно сильно отличаются.\n",
    "\n",
    "Попробуем реализовать свой собственный BL-BPE.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaeb53ca",
   "metadata": {},
   "source": [
    "Для начала реализуем базовые функции - merge, get_stats, потом добавим их в более чистую версию класса. Обучать будем на игрушечном датасете:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a26c31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"Я в своем познании настолько преисполнился, что я как будто бы уже сто триллионов миллиардов лет проживаю на триллионах и триллионах таких же планет, как эта Земля, мне этот мир абсолютно понятен, и я здесь ищу только одного - покоя, умиротворения и вот этой гармонии, от слияния с бесконечно вечным, от созерцания великого фрактального подобия и от вот этого замечательного всеединства существа, бесконечно вечного, куда ни посмотри, хоть вглубь - бесконечно малое, хоть ввысь - бесконечное большое, понимаешь? А ты мне опять со своим вот этим, иди суетись дальше, это твоё распределение, это твой путь и твой горизонт познания и ощущения твоей природы, он несоизмеримо мелок по сравнению с моим, понимаешь? Я как будто бы уже давно глубокий старец, бессмертный, ну или там уже почти бессмертный, который на этой планете от её самого зарождения, ещё когда только Солнце только-только сформировалось как звезда, и вот это газопылевое облако, вот, после взрыва, Солнца, когда оно вспыхнуло, как звезда, начало формировать вот эти коацерваты, планеты, понимаешь, я на этой Земле уже как будто почти пять миллиардов лет живу и знаю её вдоль и поперёк этот весь мир, а ты мне какие-то... мне не важно на твои тачки, на твои яхты, на твои квартиры, там, на твоё благо. Я был на этой планете бесконечным множеством, и круче Цезаря, и круче Гитлера, и круче всех великих, понимаешь, был, а где-то был конченым говном, ещё хуже, чем здесь. Я множество этих состояний чувствую. Где-то я был больше подобен растению, где-то я больше был подобен птице, там, червю, где-то был просто сгусток камня, это всё есть душа, понимаешь? Она имеет грани подобия совершенно многообразные, бесконечное множество. Но тебе этого не понять, поэтому ты езжай себе , мы в этом мире как бы живем разными ощущениями и разными стремлениями, соответственно, разное наше и место, разное и наше распределение. Тебе я желаю все самые крутые тачки чтоб были у тебя, и все самые лучше самки, если мало идей, обращайся ко мне, я тебе на каждую твою идею предложу сотню триллионов, как всё делать. Ну а я всё, я иду как глубокий старец,узревший вечное, прикоснувшийся к Божественному, сам стал богоподобен и устремлен в это бесконечное, и который в умиротворении, покое, гармонии, благодати, в этом сокровенном блаженстве пребывает, вовлеченный во всё и во вся, понимаешь, вот и всё, в этом наша разница. Так что я иду любоваться мирозданием, а ты идёшь преисполняться в ГРАНЯХ каких-то, вот и вся разница, понимаешь, ты не зришь это вечное бесконечное, оно тебе не нужно. Ну зато ты, так сказать, более активен, как вот этот дятел долбящий, или муравей, который очень активен в своей стезе, поэтому давай, наши пути здесь, конечно, имеют грани подобия, потому что всё едино, но я-то тебя прекрасно понимаю, а вот ты меня - вряд ли, потому что я как бы тебя в себе содержу, всю твою природу, она составляет одну маленькую там песчиночку, от того что есть во мне, вот и всё, поэтому давай, ступай, езжай, а я пошел наслаждаться прекрасным осенним закатом на берегу теплой южной реки. Всё, ступай, и я пойду.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e41dbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = dataset.encode(\"utf-8\") # raw bytes\n",
    "tokens = list(map(int, tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab57e52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(ids: list[int]) -> dict[tuple[int, int], int]:\n",
    "    \"\"\"counts pairs in text. Note that consecutive pairs overlap (if you have abc, pairs are ab. bc)\"\"\"\n",
    "    counts = {}\n",
    "    # collect number of token co-occurencies\n",
    "    # good way is to use zip\n",
    "    return counts\n",
    "\n",
    "get_stats(dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6736c493",
   "metadata": {},
   "source": [
    "Раскодируем самые частые пары:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38eb99d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code: get sorted dict, try some pairs, convert to bytes, than use decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5553570",
   "metadata": {},
   "source": [
    "Далее научимся мержить токены: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b4b837",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(ids: list[int], pair: tuple[int, int], new_id: int) -> list[int] :\n",
    "    \"\"\"\n",
    "    Merges all the occurencies of pair in list of ids int one new_id.\n",
    "    \"\"\"\n",
    "    new_ids = []\n",
    "    i = 0\n",
    "    while i < len(ids):\n",
    "        # check that ids[i] and [i+1] are both from pair and put the new id to list \n",
    "        # with results\n",
    "        # else just add current id to it and go to the next token\n",
    "    return new_ids\n",
    "\n",
    "print(merge([1,2,3,4,5], (2,3), 9))\n",
    "print(merge([1,2,3,4,5], (1,2), 9))\n",
    "print(merge([1,2,3,4,5], (4,5), 9))\n",
    "print(merge([1,2,3,4,5], (5,6), 9))\n",
    "print(merge([], (5,6), 9))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3b9175",
   "metadata": {},
   "source": [
    "Теперь нужно сделать прототип одной итерации алдгоритма:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141bf995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get stats, merge, add new token to vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d872394",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokens), len(tokens_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c798593f",
   "metadata": {},
   "source": [
    "И собрать все воедино:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17927315",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_orig = list(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317b8952",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_vocab_len = 100\n",
    "num_merges = # calculate how much merges are allowed\n",
    "first_merge_id = 256\n",
    "merges = {}\n",
    "for i in range(num_merges):\n",
    "    # get best pair, merge it and add the merge to the merges dict (key is pair and value is new id)\n",
    "    print(f\"New merged pair is: {next_pair}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afe5c79",
   "metadata": {},
   "source": [
    "Далее нам надо научиться кодировать и декодировать последовательности, спользуя наши мержи. Тут я перейду к классу, реализовывающему токенизатор:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2921790",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPE():\n",
    "    def __init__(self, vocab_size: int):\n",
    "        self.merges = {}\n",
    "        self.special_tokens = {}\n",
    "        self.vocab = self._build_vocab()\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "    def _build_vocab(self):\n",
    "        # дополнительный метод, который помогает построить словарь.\n",
    "        # начальный словарь - содержит все возможные символы по одному\n",
    "        vocab = {idx: bytes([idx]) for idx in range(256)} \n",
    "        # конкатенируем байты из наших мерджей\n",
    "        for (p0, p1), idx in self.merges.items():\n",
    "            vocab[idx] = vocab[p0] + vocab[p1]\n",
    "\n",
    "        # не забываем про <bos>,<eos>, <|user|>, <think> и тд.\n",
    "        for special, idx in self.special_tokens.items():\n",
    "            vocab[idx] = special.encode(\"utf-8\")\n",
    "        return vocab\n",
    "\n",
    "    def get_stats(self, ids: list[int]) -> dict[tuple[int, int], int]:\n",
    "        \"\"\"counts pairs in text. Note that consecutive pairs overlap (if you have abc, pairs are ab. bc)\"\"\"\n",
    "        # copy code here \n",
    "        return counts\n",
    "\n",
    "    def merge(self, ids: list[int], pair: tuple[int, int], new_id: int) -> list[int] :\n",
    "        \"\"\"\n",
    "        Merges all the occurencies of pair in list of ids int one new_id.\n",
    "        \"\"\"\n",
    "        # copy code here \n",
    "        return new_ids\n",
    "\n",
    "    def fit(self, dataset: list[int], verbose: bool = True) -> None:\n",
    "        # copy code here \n",
    "        for i in range(num_merges):\n",
    "            # copy code here\n",
    "            if verbose:\n",
    "                print(f\"New merged pair is: {next_pair} -> {first_merge_id + i}\")\n",
    "                \n",
    "        self.vocab = self._build_vocab()\n",
    "\n",
    "    def encode(self, text: str) -> list[int]:\n",
    "        \"\"\" Get sequence of token ids using merge dict\n",
    "        \"\"\"\n",
    "        # your code - there are several ways to encode sequence\n",
    "        # you can reuse stats and merge code (obviously if there is more than two tokens)\n",
    "        # after get of stats, you need to find first pair suitable for merges in merges dict:\n",
    "        # min(stats, key=lambda x: self.merges.get(x, float(\"inf\")))\n",
    "        # In that case, if you used all possible merges from dict, return encoded sequence.\n",
    "        \n",
    "        return tokens\n",
    "\n",
    "\n",
    "    def decode(self, token_ids: list[int]) -> str:\n",
    "        \"\"\"  \n",
    "        From list of ids we get string. Use b\"\", decode(\"utf-8\"). Use \"replace\" as param of decode.\n",
    "        \"\"\"\n",
    "        return # your code \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a81329",
   "metadata": {},
   "outputs": [],
   "source": [
    "bpe = BPE(276)\n",
    "bpe.fit(tokens_orig, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c6098f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bpe.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1f3dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to decode something"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0db4437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to encode and decode back something"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66413475",
   "metadata": {},
   "source": [
    "Мы сейчас построили самую простую версию BPE токенизатора, однако такой метод приводит к субоптимальному поведению и очень большим словарям. Поэтому многие компании расширяют его, оптимизируя некоторые части.\n",
    "Например, в cтатье [GPT-2](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) приводится, что наивная версия приводит к построению токенов из частых слов вместе со знаками препинания (dog? dog! dog.). Поэтому авторы решили добавить этап претокенизации - разбиения текста на чанки с помощью сложного regex паттерна и потом подсчету пар только внутри этих чанков.\n",
    "\n",
    "Примеры паттернов:\n",
    "- GPT2:  ```r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"```\n",
    "- GPT4: ```r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\"```\n",
    "- Qwen-7B: ```r\"\"\"(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+\"\"\"```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a4eb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "pattern = # Compile pattern\n",
    "print(re.findall(pattern, \"I'm a happy small cat in the forrest.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bc2e0f",
   "metadata": {},
   "source": [
    "###  Разбор паттерна GPT-2 (от LLM-ки)\n",
    "\n",
    "Паттерн `r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"` состоит из 5 альтернатив (разделены `|`), которые обрабатываются по порядку:\n",
    "\n",
    "1. **`'(?:[sdmt]|ll|ve|re)`** - Апострофы с английскими сокращениями:\n",
    "   - `'s`, `'d`, `'m`, `'t` (is, had, am, not)\n",
    "   - `'ll`, `'ve`, `'re` (will, have, are)\n",
    "   - Это позволяет отделить сокращения от слов\n",
    "\n",
    "2. **` ?\\p{L}+`** - Слова (буквы):\n",
    "   - ` ?` - опциональный пробел перед словом\n",
    "   - `\\p{L}+` - одна или более букв (Unicode property для всех букв)\n",
    "   - Примеры: \"hello\", \"мир\", \"你好\"\n",
    "\n",
    "3. **` ?\\p{N}+`** - Числа:\n",
    "   - ` ?` - опциональный пробел перед числом\n",
    "   - `\\p{N}+` - одна или более цифр (Unicode property)\n",
    "   - Примеры: \"123\", \"42\"\n",
    "\n",
    "4. **` ?[^\\s\\p{L}\\p{N}]+`** - Знаки препинания и специальные символы:\n",
    "   - ` ?` - опциональный пробел перед\n",
    "   - `[^\\s\\p{L}\\p{N}]+` - один или более символов, которые НЕ пробелы, НЕ буквы, НЕ цифры\n",
    "   - Примеры: \"!\", \"?\", \".\", \",\", \"@#$\"\n",
    "\n",
    "5. **`\\s+(?!\\S)|\\s+`** - Пробелы:\n",
    "   - `\\s+(?!\\S)` - пробелы, за которыми НЕ следует непробельный символ (конец строки/слова)\n",
    "   - `\\s+` - любые пробелы (fallback)\n",
    "   - Это позволяет отделить пробелы от слов\n",
    "\n",
    "**Цель**: Разделить текст на логические чанки (слова, числа, знаки препинания), чтобы BPE не создавал токены типа \"word!\" или \"word.\", а работал только внутри этих чанков.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f07971b",
   "metadata": {},
   "source": [
    "Чтобы кратенько посмотреть, как все это работает в реальных моделях (а также как обрабатываются спец-символы),  посмотрим на код [Qwen-7B](https://huggingface.co/Qwen/Qwen-7B)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cc99a5",
   "metadata": {},
   "source": [
    "### Использование tiktoken\n",
    "\n",
    "Библиотека **tiktoken** от OpenAI позволяет использовать предобученные токенизаторы различных моделей от OpenAI. Как мы видели в коде Qwen, там внутри тоже используется эта библиотека.\n",
    "\n",
    "Для быстрого примера  используем токенизатор, который используетися в GPT-4o."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eef461a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "# Загружаем токенизатор для Qwen-7B\n",
    "# tiktoken автоматически скачает необходимые файлы при первом использовании\n",
    "enc = tiktoken.get_encoding(\"o200k_base\")\n",
    "\n",
    "# Короткая строка для примера\n",
    "text = \"Привет! Hello! 你好! How are you?\"\n",
    "\n",
    "token_ids = enc.encode(text)\n",
    "print(f\"Original text: '{text}'\")\n",
    "print(f\"tokens: {token_ids}\")\n",
    "print(f\"number of tokens: {len(token_ids)}\")\n",
    "\n",
    "print(f\"\\ndecoded text '{enc.decode(token_ids)}'\")\n",
    "\n",
    "print(f\"\\ntokens as bytes:\")\n",
    "for i, token_id in enumerate(token_ids[:10]):  # показываем первые 10\n",
    "    token_bytes = enc.decode_single_token_bytes(token_id)\n",
    "    print(f\"  Token {token_id}: {token_bytes}\")\n",
    "\n",
    "special_tokens = list(enc.special_tokens_set)\n",
    "print(f\" \\nSpecial tokens: {special_tokens}\")\n",
    "# token_ids = enc.encode(special_tokens[0])\n",
    "# print(f\"tokens of first special token: {token_ids}\")\n",
    "token_ids = enc.encode(special_tokens[0], allowed_special={special_tokens[0]})\n",
    "print(f\"tokens of first special token: {token_ids}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27328c82",
   "metadata": {},
   "source": [
    "### Следующие шаги\n",
    "\n",
    "Следующее, что имеет смысл сделать - закончить реализацию токенайзера, добавив в него специальные токены и regex-паттерны. Можно попробовать обучить токенайзер с конкретным regex-паттерном на большом наборе данных (попробуйте использовать какой-нибудь открытый датасет или несколько) и проверить, насколько ваш токенизатор пересекается с токенизатором выбранной модели.\n",
    "\n",
    "Далее, по поведению модели относительно некооторых токенов можно понять, на чсем она обучалась, например, есть интересный [блок-пост](https://fi-le.net/oss/) и сопроводительный код к нему на [гитхабе](https://github.com/lennart-finke/gpt-oss/). Еще один пост: [про китайские токены в gpt-oss](https://www.technologyreview.com/2024/05/17/1092649/gpt-4o-chinese-token-polluted/). Попробуйте проанализиировать какую-нибудь модель - какие токены аномальны для нее? А, может, вы сможете проверить русские токены в токенизаторе?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715ea309",
   "metadata": {},
   "source": [
    "## А что с изображениями:\n",
    "Теперь поговорим о том, как можно представить в виде токенов картинку.  Vision Transformer (ViT) разбивает изображение на патчи (patches), которые можно рассматривать как токены изображения.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac89823",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import ViTImageProcessor, ViTModel\n",
    "import requests\n",
    "from io import BytesIO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c957424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загружаем предобученный процессор и модель ViT из Hugging Face\n",
    "processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\n",
    "model = ViTModel.from_pretrained('google/vit-base-patch16-224')\n",
    "\n",
    "# Получаем параметры из конфигурации модели\n",
    "patch_size = model.config.patch_size  # размер патча (16 для patch16 моделей)\n",
    "image_size = processor.size['height']  # размер изображения (224)\n",
    "\n",
    "print(f\"Размер патча: {patch_size}x{patch_size} пикселей\")\n",
    "print(f\"Размер изображения: {image_size}x{image_size} пикселей\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79147d0d",
   "metadata": {},
   "source": [
    "## Vision Transformer (ViT)\n",
    "\n",
    "**Vision Transformer (ViT)** - это трансформерная архитектурадля обработки изображений. Впервые предложена в статье [\"An Image is Worth 16x16 Words\"](https://arxiv.org/abs/2010.11929) в 2020 году.\n",
    "Основная идея заключается в следующем:\n",
    "1) Изображение разбивается на небольшие квадратные патчи, которые рассматриваются как \"токены\" изображения.\n",
    "2) Каждый патч сверткой вытягивается в одномерный вектор и проходит через линейный слой (embedding layer), превращаясь в эмбеддинг фиксированной размерности.\n",
    "3) **[CLS] токен**: В начало последовательности добавляется специальный токен [CLS] (classification token), который после прохождения через модель содержит информацию обо всем изображении и используется для классификации (подробнее на лекциях и практике по BERT).\n",
    "4) Последовательность эмбеддингов патчей проходит через стандартный Transformer Encoder (подробнее на лекциях и практике по BERT).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb9be14",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "image = Image.open(\"..\\images\\kot_mjaukaet.jpg\").convert('RGB')\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(image)\n",
    "plt.axis('off')\n",
    "plt.title('Исходное изображение')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca2d6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "patch_size = model.config.patch_size  # размер патча (16 для patch16 моделей)\n",
    "image_size = processor.size['height']  # размер изображения (224)\n",
    "\n",
    "num_patches_per_side = image_size // patch_size\n",
    "total_patches = num_patches_per_side ** 2\n",
    "\n",
    "print(f\"Изображение разбито на {num_patches_per_side}x{num_patches_per_side} = {total_patches} патчей\")\n",
    "print(f\"Размер каждого патча: {patch_size}x{patch_size} пикселей\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da3c835",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_patches(image, patch_size=16, num_patches_per_side=14):\n",
    "    img_array = np.array(image.resize((224, 224)))\n",
    "    fig, axes = plt.subplots(\n",
    "        num_patches_per_side, num_patches_per_side, figsize=(20, 20)\n",
    "    )\n",
    "    fig.suptitle('Токены изображения (патчи 16x16)', fontsize=16)\n",
    "    for i in range(num_patches_per_side):\n",
    "        for j in range(num_patches_per_side):\n",
    "            y_start = i * patch_size\n",
    "            y_end = y_start + patch_size\n",
    "            x_start = j * patch_size\n",
    "            x_end = x_start + patch_size\n",
    "            \n",
    "            patch = img_array[y_start:y_end, x_start:x_end]\n",
    "            \n",
    "            axes[i, j].imshow(patch)\n",
    "            axes[i, j].axis('off')\n",
    "            axes[i, j].set_title(f'({i},{j})', fontsize=6)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return img_array\n",
    "\n",
    "patched_image = visualize_patches(image, patch_size=patch_size, num_patches_per_side=num_patches_per_side)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
