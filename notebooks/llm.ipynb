{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7ceb3b8",
   "metadata": {},
   "source": [
    "# GPT-2: Введение и претрейн\n",
    "\n",
    "В этом ноутбуке мы изучим архитектуру GPT-2 (Generative Pre-trained Transformer 2) и научимся претрейнить небольшую языковую модель с нуля.\n",
    "\n",
    "GPT-2 - это авторегрессионная языковая модель, которая была представлена OpenAI в 2019 году. Она основана на архитектуре трансформера и использует только декодерную часть (decoder-only), что делает её идеальной для задач генерации текста.\n",
    "\n",
    "**План на сегодня: **\n",
    "- Архитектура GPT-2 и принципы работы decoder-only трансформеров\n",
    "- Использование предобученных моделей\n",
    "- Подготовка данных для претрейнинга\n",
    "- Полнуя реализация процесса обучения языковой модели\n",
    "- Оценка качества обученной модели\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a62b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "import numpy as np\n",
    "from transformers import (\n",
    "    GPT2LMHeadModel,\n",
    "    GPT2Tokenizer,\n",
    "    GPT2Config,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    get_cosine_schedule_with_warmup)\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "current_dir = Path.cwd()\n",
    "\n",
    "\n",
    "if current_dir.name == 'notebooks':\n",
    "    project_dir = current_dir.parent  # E:\\projects\\spbu_dl_2025\n",
    "    parent_dir = project_dir.parent   # E:\\projects\n",
    "else:\n",
    "    project_dir = current_dir\n",
    "    parent_dir = current_dir.parent\n",
    "\n",
    "if str(parent_dir) not in sys.path:\n",
    "    sys.path.insert(0, str(parent_dir))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6404c491",
   "metadata": {},
   "source": [
    "## 1. Архитектура GPT-2\n",
    "\n",
    "GPT-2 использует архитектуру **decoder-only трансформера**. Это означает, что модель состоит только из декодерных слоев, в отличие от:\n",
    "- **Encoder-only** (BERT): использует только энкодерные слои, подходит для задач понимания текста\n",
    "- **Encoder-Decoder** (T5, BART): использует оба компонента, подходит для задач перевода и summarization\n",
    "\n",
    "Вопросы:\n",
    "\n",
    "- Почему GPT-2 использует causal masking, а не bidirectional attention как BERT?\n",
    "- Какие преимущества дает decoder-only архитектура для генерации текста?\n",
    "- В чем разница между self-attention в GPT-2 и cross-attention в encoder-decoder моделях?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7c4ebe",
   "metadata": {},
   "source": [
    "## 2. Токенизация\n",
    "\n",
    "GPT-2 использует **Byte Pair Encoding (BPE)** токенизацию. Это позволяет модели работать с подсловными единицами, что особенно полезно для редких слов и различных языков.\n",
    "\n",
    "### Особенности GPT-2 токенизации:\n",
    "\n",
    "- Использует BPE с претокенизацией (разбиение текста на чанки перед применением BPE)\n",
    "- Словарь размером 50,257 токенов\n",
    "- Специальные токены: `<|endoftext|>` для обозначения конца текста\n",
    "- Не использует токены `[CLS]` и `[SEP]` как BERT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8283fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# GPT-2 не имеет pad_token по умолчанию, устанавливаем его\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"Специальные токены:\")\n",
    "print(f\"pad_token: {tokenizer.pad_token}\")\n",
    "print(f\"eos_token: {tokenizer.eos_token}\")\n",
    "print(f\"bos_token: {tokenizer.bos_token}\")\n",
    "print(f\"unk_token: {tokenizer.unk_token}\")\n",
    "print(f\"\\nРазмер словаря: {tokenizer.vocab_size}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefdb823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Примеры токенизации\n",
    "texts = [\n",
    "    \"Hello, world!\",\n",
    "    \"Привет, мир!\",\n",
    "    \"I like to eat apples.\",\n",
    "    \"Сколько букв в слове амальгамма?\"\n",
    "]\n",
    "\n",
    "for text in texts:\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    token_ids = tokenizer.encode(text)\n",
    "    decoded = tokenizer.decode(token_ids)\n",
    "    \n",
    "    print(f\"\\nТекст: {text}\")\n",
    "    print(f\"Токены: {tokens}\")\n",
    "    print(f\"ID токенов: {token_ids}\")\n",
    "    print(f\"Декодированный: {decoded}\")\n",
    "    print(\"-\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068676dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загружаем предобученную модель GPT-2 (small версия)\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "model.to(device)\n",
    "model.eval()  # Переводим в режим оценки\n",
    "\n",
    "print(f\"Количество параметров: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Размер модели: {sum(p.numel() * 4 for p in model.parameters()) / 1024 / 1024:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54fe500",
   "metadata": {},
   "source": [
    "Размеры моделей GPT-2:\n",
    "\n",
    "| Модель | Слои | Головы внимания | Размерность эмбеддингов | Параметры |\n",
    "|--------|------|-----------------|------------------------|-----------|\n",
    "| **GPT-2** | 12 | 12 | 768 | 124M |\n",
    "| **GPT-2 Medium** | 24 | 16 | 1024 | 355M |\n",
    "| **GPT-2 Large** | 36 | 20 | 1280 | 774M |\n",
    "| **GPT-2 XL** | 48 | 25 | 1600 | 1.5B |\n",
    "\n",
    "Параметры конкретно нашей модельки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f546743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Просмотр конфигурации GPT-2 124B\n",
    "config = model.config\n",
    "print(\"Конфигурация GPT-2:\")\n",
    "print(f\"vocab_size: {config.vocab_size}\")\n",
    "print(f\"n_positions: {config.n_positions} (максимальная длина последовательности)\")\n",
    "print(f\"n_ctx: {config.n_ctx} (контекстное окно)\")\n",
    "print(f\"n_embd: {config.n_embd} (размерность эмбеддингов)\")\n",
    "print(f\"n_layer: {config.n_layer} (количество трансформерных слоев)\")\n",
    "print(f\"n_head: {config.n_head} (количество голов внимания)\")\n",
    "print(f\"n_inner: {config.n_inner} (размерность внутреннего слоя FFN)\")\n",
    "print(f\"activation_function: {config.activation_function}\")\n",
    "print(f\"layer_norm_epsilon: {config.layer_norm_epsilon}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b32c3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3f241b",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5231dbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Простая генерация текста\n",
    "def generate_text(prompt, max_length=50, temperature=1.0, top_k=50, top_p=0.95, do_sample=True):\n",
    "    inputs = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs,\n",
    "            max_length=max_length,\n",
    "            temperature=temperature,\n",
    "            top_k=top_k,\n",
    "            top_p=top_p,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            do_sample=do_sample\n",
    "        )\n",
    "    \n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "# Пример генерации\n",
    "prompt = \"Cats are very cute animals.\"\n",
    "print(\"Сгенерированный текст:\")\n",
    "print(generate_text(prompt, max_length=80))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b46d8fc",
   "metadata": {},
   "source": [
    "Параметры генерации:\n",
    "\n",
    "- **temperature**: контролирует случайность генерации (выше = более случайно, ниже = более детерминированно)\n",
    "- **top_k**: ограничивает выбор следующих токенов только k наиболее вероятными\n",
    "- **top_p (nucleus sampling)**: выбирает токены из минимального набора, сумма вероятностей которых >= p\n",
    "\n",
    "Попробуем изменить параметры генерации и посмотрим, как это влияет на результат:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f290a517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Экспериментируйте с параметрами\n",
    "prompt = \"Once upon a time\"\n",
    "\n",
    "print(\"Temperature = 0.5 (более детерминированно):\")\n",
    "print(generate_text(prompt, max_length=60, temperature=0.5))\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "print(\"Temperature = 1.5 (более случайно):\")\n",
    "print(generate_text(prompt, max_length=60, temperature=1.5))\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "print(\"Top-k = 10 (более ограниченный выбор):\")\n",
    "print(generate_text(prompt, max_length=60, top_k=10))\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "print(\"Top-p = 0.5 (nucleus sampling):\")\n",
    "print(generate_text(prompt, max_length=60, top_p=0.5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3dc6c40",
   "metadata": {},
   "source": [
    "Вопросы:\n",
    "\n",
    "- Как увеличение количества слоев влияет на способности модели?\n",
    "- Почему размерность эмбеддингов обычно кратна количеству голов внимания?\n",
    "- Какие компромиссы существуют между размером модели и скоростью обучения/инференса?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741b6a55",
   "metadata": {},
   "source": [
    "## GPT-2 с нуля"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60a07cb",
   "metadata": {},
   "source": [
    "Для начала рассмотрим исходную архитектуру трансформера:\n",
    "\n",
    "<img src='../images/transformer.png' width='500px' />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627599a3",
   "metadata": {},
   "source": [
    "Начнем реализовывать нашу модельку, для начала повторим структуру модели из huggingface, чтобы 1) научиться генерировать 2) удостовериться, что наша модель будет работать как ожидается, когда мы ее обучим. 3) Не обучать ее полностью с нуля далее. \n",
    "\n",
    "Отличия модели от исходной архитектуры Transformer:\n",
    "1) Только decoder-часть\n",
    "2) Используется pre-normalization, не post-normalization. Это помогает в обучении, так как стабилизирует градиенты за счет более сбалансированного вклада residual ветки и основной в граф. Кроме того, в таком случае на вход MLP и Attention приходит отнормализованный сигнал, что также улучшает стабильность.\n",
    "3) В качестве активации используется [GeLU](https://arxiv.org/pdf/1606.08415), так как она решает проблему мертвых градиентов в ReLU, дифференцируема везде и немного помогает отрегуляризировать модель\n",
    "Формула GeLU: $f(x) = x · \\Phi(x) $, где $\\Phi(x)$ - CDF распределения Гаусса\n",
    "\n",
    "Особенности:\n",
    "- для полного воспроизведения результатов используется старая верся GeLU, которая основана на аппроксимации с помощью [tanh](https://docs.pytorch.org/docs/stable/generated/torch.nn.GELU.html). \n",
    "- Можете изучить исходный код [huggingface](https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py), который во многом повторяется здесь."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89937da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    vocab_size: int = 50257 # почему именно 50257?\n",
    "    block_size: int = 1024 # corresponding to max sequence length\n",
    "    n_layer: int = 12 # number of layers\n",
    "    n_head: int = 12 # number of attention heads\n",
    "    n_embed: int = 768 # embedding dimension\n",
    "    activation_function: str = \"gelu\" # activation function\n",
    "    layer_norm_epsilon: float = 1e-5 # epsilon for layer normalization\n",
    "    use_torch_attention: bool = False\n",
    "    n_positions: int = 1024 # maximum sequence length\n",
    "\n",
    "\n",
    "# Если вы решите учить модель с нуля, можете сравнить разные функции активации\n",
    "activations = {\n",
    "    \"gelu\": nn.GELU(),\n",
    "    \"relu\": nn.ReLU(),\n",
    "    \"silu\": nn.SiLU(),\n",
    "    \"swi\": nn.SiLU(),\n",
    "    \"gelu_old\": nn.GELU(approximate=\"tanh\"),\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88efa438",
   "metadata": {},
   "source": [
    "Для начала создадим скелет модели. GPT-2 представляет собой достаточно типичную структуру, в которой последовательно повторяется некоторое количество одинаковых трансформерных блоков. Названия слоев и блоков, а также все параметры должны быть такими же, как у предобученной модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d9f76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spbu_dl_2025.utils.llm import sample_next_token\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.use_torch_attention = config.use_torch_attention\n",
    "        # wpe means positional encoding, wte means token embedding (output embedding), h means hidden layers, ln_f means final layer norm\n",
    "        self.transformer = nn.ModuleDict({\n",
    "            'wpe': # создайте слой эмбеддинга для кодирования позиций,\n",
    "            'wte': #  создайте слой эмбеддингов для кодирования токенов,\n",
    "            'h': # создайте внутреннюю часть (последовательность слоев TransformerBlock) с помощью nn.ModuleList\n",
    "            'ln_f': nn.LayerNorm(config.n_embed, eps=config.layer_norm_epsilon),\n",
    "        })\n",
    "        self.lm_head = #Cоздайте голову для определения логитов токеноы\n",
    "\n",
    "        # weight sharing scheme (reduces 768*50267=~40M params, fewer params, more efficient)\n",
    "        self.transformer.wte.weight = self.lm_head.weight\n",
    "        \n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "\n",
    "        _, seq_len = idx.size()\n",
    "        assert seq_len <= self.config.block_size, f\"Cannot forward sequence of length {T}, block size is only {self.config.block_size}\"\n",
    "\n",
    "        pos = # Создайте входы для позиционных эмбеддингов. Позиционные эмбеддинги принимают на вход номера токенов в последовательности\n",
    "        pos_emd = self.transformer.wpe(pos)\n",
    "        tok_emd = self.transformer.wte(idx)\n",
    "\n",
    "        x = # Сложите эмбеддинги\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "\n",
    "        # For model train\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            # Зачем необходимо использование view в этой части?\n",
    "            loss = F.cross_entropy(self.lm_head(x).view(-1, self.config.vocab_size), targets.view(-1))\n",
    "            \n",
    "        return self.lm_head(x), loss\n",
    "\n",
    "    def generate(\n",
    "        self, \n",
    "        x, \n",
    "        max_length, \n",
    "        do_sample=True, \n",
    "        top_k=None, \n",
    "        top_p=None, \n",
    "        temperature=1.0,\n",
    "        pad_token_id=None,\n",
    "        eos_token_id=None,\n",
    "        ):\n",
    "\n",
    "        while x.size(1) < max_length:\n",
    "            if x.size(1) >= self.config.block_size:\n",
    "                break\n",
    "            logits, _ = self.forward(x)\n",
    "            logits = # возьмите логиты только последнего токена\n",
    "            new_token = sample_next_token(logits, do_sample, top_k, top_p, temperature)\n",
    "                \n",
    "            x = torch.cat([x, new_token], dim=1)\n",
    "            # Check if any token in the batch is EOS token\n",
    "            if eos_token_id is not None and (new_token == eos_token_id).any():\n",
    "                # EOS stopping criterion\n",
    "                break\n",
    "            # Опционально: реализуйте генерацию в батче\n",
    "        return x\n",
    "\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            std = 0.02\n",
    "            if hasattr(module, 'SCALE_INIT'):\n",
    "                std /= (2 * self.config.num_layers)**0.5\n",
    "            torch.nn.init.normal_(module.weight, mean=0, std=std)    # as per openai gpt-2 source code\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0, std=0.02)\n",
    "    \n",
    "    \n",
    "    @classmethod\n",
    "    def from_pretrained(cls, model_type, use_torch_attention: bool = False, use_grad_checkpoint: bool = False):\n",
    "        \"\"\"Loads pretrained weights from Hugging Face.\"\"\"\n",
    "        assert model_type in {\"gpt2\", \"gpt2-medium\", \"gpt2-large\", \"gpt2-xl\"}\n",
    "        print(\"loading weights from pretrained gpt: %s\" % model_type)\n",
    "\n",
    "        # n_layer, n_head and n_embd are determined from model_type\n",
    "        config_args = {\n",
    "            \"gpt2\": dict(n_layer=12, n_head=12, n_embed=768),  # 124M params\n",
    "            \"gpt2-medium\": dict(n_layer=24, n_head=16, n_embed=1024),  # 350M params\n",
    "            \"gpt2-large\": dict(n_layer=36, n_head=20, n_embed=1280),  # 774M params\n",
    "            \"gpt2-xl\": dict(n_layer=48, n_head=25, n_embed=1600),  # 1558M params\n",
    "        }[model_type]\n",
    "        config_args[\"vocab_size\"] = 50257  # always 50257 for GPT model checkpoints\n",
    "        config_args[\"block_size\"] = 1024  # always 1024 for GPT model checkpoints\n",
    "        config_args[\"use_torch_attention\"] = use_torch_attention\n",
    "\n",
    "        config = GPTConfig(**config_args)\n",
    "        model = GPT(config)\n",
    "        sd = model.state_dict()\n",
    "        sd_keys = sd.keys()\n",
    "        sd_keys = [k for k in sd_keys if not k.endswith(\".attn.bias\")]  # discard this mask / buffer, not a param\n",
    "\n",
    "        # init a huggingface/transformers model\n",
    "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
    "        sd_hf = model_hf.state_dict()\n",
    "        # copy while ensuring all of the parameters are aligned and match in names and shapes\n",
    "        sd_keys_hf = sd_hf.keys()\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith(\".attn.masked_bias\")]  # ignore these, just a buffer\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith(\".attn.bias\")]  # same, just the mask (buffer)\n",
    "        transposed = [\"attn.c_attn.weight\", \"attn.c_proj.weight\", \"mlp.c_fc.weight\", \"mlp.c_proj.weight\"]\n",
    "        # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n",
    "        # this means that we have to transpose these weights when we import them\n",
    "        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
    "        for k in sd_keys_hf:\n",
    "            if any(k.endswith(w) for w in transposed):\n",
    "                # special treatment for the Conv1D weights we need to transpose\n",
    "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k].t())\n",
    "            else:\n",
    "                # vanilla copy over the other parameters\n",
    "                assert sd_hf[k].shape == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k])\n",
    "\n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуем сам TransformerBlock и MLP:\n",
    "Устройство TransformerBlock аналогично базовому блоку (Однако, по крайней мере в transformers, используется нормализация до подачи в аттеншен, а не после)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37751662",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embed, eps=config.layer_norm_epsilon)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embed, eps=config.layer_norm_epsilon)\n",
    "        self.mlp = MLP(config)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # normalization is applied before the attention and mlp\n",
    "        # this helps to stabilize the training because the gradients are less likely to explode\n",
    "        # it also helps to improve the performance of the model because inputs \n",
    "        # of attention and mlp are normalized\n",
    "\n",
    "        x = # Реализуйте аттеншен часть с пренормализацией\n",
    "        x = # Реализуйте линейную часть с пренормализацией\n",
    "        return x\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        self.c_fc = # Линейный слой с 4х-кратным расширением\n",
    "        self.c_proj =# Линейный слой с 4х-кратным сужением\n",
    "        # self.act = activations[config.activation_function] - на будущее\n",
    "        self.gelu = nn.GELU(approximate=\"tanh\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.c_proj(self.gelu(self.c_fc(x))) # для загрузки весов\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03f2fda",
   "metadata": {},
   "source": [
    "Последняя фундаментальная часть - слой Attention, в нашем случае Multihead Self Attention. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        assert config.n_embed % config.n_head == 0, \"n_embed must be divisible by n_head to make the heads equal size\"\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embed = config.n_embed\n",
    "        self.c_attn = nn.Linear(config.n_embed, 3 * config.n_embed) # 3 * n_embed because we have 3 submatrices: query, key, value\n",
    "        self.c_proj = nn.Linear(config.n_embed, config.n_embed) \n",
    "        self.use_torch_attention = config.use_torch_attention\n",
    "        # In openai it was \"bias\" so i need to use that name, but actually it's a mask\n",
    "        self.register_buffer('bias', \n",
    "        torch.triu(\n",
    "            # создайте единичную марицу типа bool размера block_size x block size, замените верхний трегольник на нули.\n",
    "        ).view(1, 1, config.block_size, config.block_size)\n",
    "    ) # 4 dimensions for mask are: (batch, head, tokens, tokens), and one mask is used for all heads and all sequence\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch, tokens, emb_dim = x.size() # batch size, sequence length, embedding dimensionality (n_embed)    \n",
    "\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        # question: why we need to use one c_attn to calculate q,k,v? \n",
    "        q, k, v = self.c_attn(x).split(self.n_embed, dim=2)\n",
    "        # question: why we need to split the input by n_embed?\n",
    "        k = k.view(batch, tokens, self.n_head, emb_dim // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(batch, tokens, self.n_head, emb_dim // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(batch, tokens, self.n_head, emb_dim // self.n_head).transpose(1, 2) # (B, nh, T, hs)    \n",
    "\n",
    "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        if self.use_torch_attention:  \n",
    "            # efficient attention using Flash Attention if it's available\n",
    "            out = F.scaled_dot_product_attention(q, k, v, attn_mask=None)\n",
    "        else:\n",
    "            # question: why we need to scale the attention?\n",
    "            att = # Посчитайте (q * k^T) / sqrt(d)\n",
    "            att = # примените каузальную маску, заполните скоры \"-inf\"\n",
    "            att = # примените софтмакс\n",
    "            out = # Посчитайте финальные скоры, умножив out на v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        \n",
    "        out = out.transpose(1, 2).contiguous().view(batch, tokens, self.n_embed) # we \"concatenate\" all heads outputs side by side\n",
    "\n",
    "        # output projection\n",
    "        return self.c_proj(out) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4704de5",
   "metadata": {},
   "source": [
    "Проверим нашу модельку:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18224ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fda102",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT.from_pretrained(\"gpt2\")\n",
    "model.eval()\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccff72a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Top-k = 20:\")\n",
    "prompt = \"Where is Saint Petersburg?\"\n",
    "print(generate_text(prompt, max_length=60, top_k=20))\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "print(generate_text(prompt, max_length=60, top_k=20))\n",
    "\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "print(generate_text(prompt, max_length=60, top_k=20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b757af",
   "metadata": {},
   "source": [
    "Итак, мы смогли создать модель, которая может генерировать текст, теперь нам нужно научиться ее обучать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf4774d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Top-k = 20:\")\n",
    "prompt = \"Где находится Москва?\"\n",
    "print(generate_text(prompt, max_length=60, top_k=20))\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "print(generate_text(prompt, max_length=60, top_k=20))\n",
    "\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "print(generate_text(prompt, max_length=60, top_k=20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c21cb6",
   "metadata": {},
   "source": [
    "## 5. Претрейн\n",
    "В этом ноутбуке мы не будем на самом деле претрейнить модель, но мы попробуем просимулировать это на небольшом корпусе русского языка, инициализировав модель >!квеном!<.\n",
    "Я взяла [artifitial dostojevsky](https://gitlab.com/z00logist/artificial-dostoevsky/-/blob/main/data/corpus.txt?ref_type=heads), но можно использовать и, например, другие корпусы:\n",
    "- [стихотворения Пушкина](https://dataverse.pushdom.ru/dataset.xhtml?persistentId=doi:10.31860/openlit-2023.8-C005)\n",
    "- [финансовые новости](https://www.kaggle.com/datasets/kkhubiev/russian-financial-news)\n",
    "- [19 000 Russian poems](https://www.kaggle.com/datasets/grafstor/19-000-russian-poems)\n",
    "- [Russian Novels](https://github.com/JoannaBy/RussianNovels/tree/master) - классические произведения (около 100)\n",
    "\n",
    "и другие..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feaa49bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(r\"E:\\projects\\spbu_dl_2025\\notebooks\\data\\corpus.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    corpus = f.read()\n",
    "corpus[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e87850",
   "metadata": {},
   "source": [
    "Как будем учить? У нас есть единый корпус, но нам надо:\n",
    "1) разбить его на входы и лейблы\n",
    "3) собрать в батчи\n",
    "4) настроить оптимизатор, шедулер и лосс\n",
    "5) собственно, подождать N часов/дней, пока модель обучится\n",
    "\n",
    "Как выглядят входы (x) и лейблы (y) модели:\n",
    "x – фрагмент последовательности (токенов), начинающийся с любого индекса всей обучающей последовательности и размером seq_len.\n",
    "y – это тот же x, только смещенный на одну позицию вправо:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca309392",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tokenizer.encode(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cc5c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(corpus)/1000000, len(data)/1000000, \"M\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204c5373",
   "metadata": {},
   "source": [
    "Проверим обучение на 1 батче:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc69fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "seq_len = 1024\n",
    "num_training_steps = 50\n",
    "learning_rate = 5e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7112176",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(data[0: seq_len * batch_size]).view(batch_size, seq_len).to(device)\n",
    "y = torch.tensor(data[1: seq_len * batch_size+1]).view(batch_size, seq_len).to(device)\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daba82c3",
   "metadata": {},
   "source": [
    "Для оценки моделей часто используется perplexity. Эта функция определяется как экспонента от среднего NLL последовательности (т.е. нашей функции потерь). Интуитивный смысл этой функции в том, что она показывает, насколько уверенно модель предсказывает следующий токен."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2aaf34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perplexity(loss):\n",
    "    \"\"\"Вычисление perplexity из loss\"\"\"\n",
    "    return math.exp(loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a34dbd",
   "metadata": {},
   "source": [
    "Далее начинается уже знакомый нам пайплайн обучения, так как задача фактически свелась к бинарной классификации. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63380937",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Выберите тип scheduler:\n",
    "# Вариант 1: Linear schedule\n",
    "# scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=100, num_training_steps=num_training_steps)\n",
    "\n",
    "# Вариант 2: Cosine schedule (рекомендуется для длительного обучения)\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer, \n",
    "    num_warmup_steps=100, \n",
    "    num_training_steps=num_training_steps,\n",
    "    num_cycles=0.5  # Половина цикла косинуса\n",
    ")\n",
    "\n",
    "for i in range(num_training_steps):\n",
    "    optimizer.zero_grad()\n",
    "    logits, loss = model(x, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    optimizer.zero_grad()\n",
    "    print(f\"Step {i} | loss: {loss.item()}, perplexity: {calculate_perplexity(loss.item())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d905a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Ох уж мне эти сказочники!\"\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "print(generate_text(prompt, max_length=128, top_k=20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265ad980",
   "metadata": {},
   "source": [
    "Обучение мы будем проводить с помощью Pytorch, как мы это уже делали раньше. Поэтому нам снова нужно подготовить класс датасета, который будет отдавать один семпл данных. \n",
    "Для нас разумно разбивать данные с перекрытием, а именно считать индекс семпла как индекс его первого токена, и отдавать семпл по этому индексу. Тогда, если мы не зайдем за край датасета, мы сможем наиболее эффективно его использовать.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3519e60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PretrainData(Dataset):\n",
    "    \"\"\"\n",
    "    Создает пары (x, y) из последовательности данных, где:\n",
    "    - x: последовательность длиной seq_len, начиная с позиции idx\n",
    "    - y: последовательность длиной seq_len, начиная с позиции idx + 1 (сдвиг на 1)\n",
    "    \"\"\"\n",
    "    def __init__(self, data: list[int], seq_len: int, device: str):\n",
    "        self.data = data\n",
    "        self.seq_len = seq_len\n",
    "        self.device = device\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.seq_len - 1 # нельзя заходить за край датасета\n",
    "    \n",
    "    def __getitem__(self, idx: int):\n",
    "        x = # создайте long tensor из данных размера seq_len, начиная с idx\n",
    "        y = # создайте long tensor из данных размера seq_len, начиная с idx + 1\n",
    "        return (x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f0970b",
   "metadata": {},
   "source": [
    "Вы можете попробовать обучить модель совсем с нуля, для этого вам понадобятся гораздо более крупные датасеты (и гораздо больше ресурсов). Вот пара примеров:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0554a699",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "\n",
    " from datasets import load_dataset\n",
    "\n",
    "\n",
    "ds = load_dataset(\"HuggingFaceTB/smollm-corpus\", \"cosmopedia-v2\", split=\"train\", num_proc=16)\n",
    "print(ds[0])\n",
    "# очень большой датасет, только для смелых\n",
    "# ds = load_dataset(\"HuggingFaceTB/smollm-corpus\", \"fineweb-edu-dedup\", split=\"train\", num_proc=16)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3b810d59",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Теперь можно создать dataloader-ы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1daf077f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = PretrainData(data, seq_len=1024, device=device)\n",
    "# Разделяем на train и validation\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "    dataset, [train_size, val_size]\n",
    ")\n",
    "batch_size=4\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "print(f\"Train размер: {len(train_dataset)}\")\n",
    "print(f\"Validation размер: {len(val_dataset)}\")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Train число батчей: {len(train_loader)}\")\n",
    "print(f\"Validation число батчей: {len(val_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ce0265",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d2e7da37",
   "metadata": {},
   "source": [
    "\n",
    "Для обучения языковых моделей часто используются два типа learning rate schedulers:\n",
    "\n",
    "1. **Linear Schedule** (`get_linear_schedule_with_warmup`):\n",
    "   - Линейно уменьшает learning rate от максимального значения до 0\n",
    "   - Простая и стабильная стратегия\n",
    "   - Хорошо работает для короткого обучения\n",
    "\n",
    "2. **Cosine Schedule** (`get_cosine_schedule_with_warmup`):\n",
    "   - Уменьшает learning rate по косинусоиде\n",
    "   - Более плавное уменьшение в начале, более быстрое в конце\n",
    "   - Часто дает лучшие результаты для длительного обучения\n",
    "   - Параметр `num_cycles` контролирует количество циклов косинуса:\n",
    "     - `num_cycles=0.5` (по умолчанию) - половина цикла (от 0 до π)\n",
    "     - `num_cycles=1.0` - полный цикл (от 0 до 2π)\n",
    "     - `num_cycles=1.5` - полтора цикла\n",
    "\n",
    "**Warmup** - это период, в течение которого learning rate постепенно увеличивается от 0 до максимального значения. Это помогает стабилизировать обучение в начале. (У нас обучение идет не с начала, но мы все равно настроим его)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e175ec8b",
   "metadata": {},
   "source": [
    "В этом ноутбуке предполагается некотое количество экспериментов и абляций, поэтому настраиваем ClearML (альтернатива wandb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7e5d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env CLEARML_WEB_HOST=https://app.clear.ml/\n",
    "%env CLEARML_API_HOST=https://api.clear.ml\n",
    "%env CLEARML_FILES_HOST=https://files.clear.ml\n",
    "%env CLEARML_API_ACCESS_KEY=#your_key\n",
    "%env CLEARML_API_SECRET_KEY=#your_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65122424",
   "metadata": {},
   "outputs": [],
   "source": [
    "from clearml import Task\n",
    "task = Task.init(project_name=\"gpt2-pretrain\", task_name=\"Experiment Run 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb0f632",
   "metadata": {},
   "source": [
    "Зададим параметры обучения и модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0b50cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "params={\n",
    "    \"batch_size\": batch_size,\n",
    "    \"num_epochs\": 2,\n",
    "    \"seq_len\": seq_len,\n",
    "    \"learning_rate\": learning_rate,\n",
    "    \"num_training_steps\": num_training_steps,\n",
    "    \"num_warmup_steps\": 100,\n",
    "    \"optimizer\": \"AdamW\",\n",
    "    \"scheduler\": \"cosine_with_warmup\",\n",
    "    \"gradient_accumulation\": 4,\n",
    "    \"learning_rate\": 5e-4,\n",
    "    \"num_warmup_steps\": 1000,\n",
    "    \"use_torch_attention\": True,\n",
    "    \"use_grad_checkpoint\": True\n",
    "\n",
    "}\n",
    "\n",
    "task.connect(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65256638",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT.from_pretrained(\"gpt2\", use_torch_attention=params[\"use_torch_attention\"], use_grad_checkpoint=params[\"use_grad_checkpoint\"])\n",
    "model.eval()\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ba10f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Настройка обучения\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Используемое устройство: {device}\")\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# Оптимизатор\n",
    "learning_rate = params[\"learning_rate\"]\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Learning rate scheduler\n",
    "num_epochs = params[\"num_epochs\"]\n",
    "num_training_steps = len(train_loader) * num_epochs\n",
    "num_warmup_steps = params[\"num_warmup_steps\"]\n",
    "\n",
    "if params[\"scheduler\"] == \"linear_with_warmup\":\n",
    "    # Linear schedule (линейное уменьшение learning rate)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=num_warmup_steps,\n",
    "        num_training_steps=num_training_steps\n",
    "    )\n",
    "elif params[\"scheduler\"] == \"cosine_with_warmup\":\n",
    "    # Cosine schedule уменьшает learning rate по косинусоиде, что часто дает лучшие результаты\n",
    "    # Особенно полезно для длительного обучения\n",
    "    scheduler = get_cosine_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=num_warmup_steps,\n",
    "        num_training_steps=num_training_steps,\n",
    "        num_cycles=0.5  # Количество полных циклов косинуса (0.5 = половина цикла)\n",
    "    )\n",
    "else:\n",
    "    raise ValueError(\"scheduler is not configured\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69a115c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для валидации\n",
    "def validate(model, val_loader, device,  max_batches: int = 100):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_id, batch in enumerate(val_loader):\n",
    "            if batch_id > max_batches:\n",
    "                break\n",
    "            inputs, targets = batch\n",
    "\n",
    "            preds, loss = model(inputs, targets=targets)\n",
    "            \n",
    "            # Учитываем только не-padding токены\n",
    "            mask = (targets != tokenizer.pad_token_id).float()\n",
    "            num_tokens = mask.sum().item()\n",
    "            \n",
    "            total_loss += loss.item() * num_tokens\n",
    "            total_tokens += num_tokens\n",
    "    \n",
    "    avg_loss = total_loss / total_tokens if total_tokens > 0 else 0\n",
    "    perplexity = calculate_perplexity(avg_loss)\n",
    "    \n",
    "    return avg_loss, perplexity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b458ac",
   "metadata": {},
   "source": [
    "Собираем все вместе:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c0c8f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_exp(\n",
    "    exp_name, \n",
    "    task,\n",
    "    batch_size: int = 4, \n",
    "    gradient_accumulation: int = 4, \n",
    "    num_warmup_steps: int = 100, \n",
    "    scheduler: str = \"cosine_with_warmup\",\n",
    "    use_torch_attention: bool = True,\n",
    "    use_grad_checkpoint: bool = False,\n",
    "    max_batches: int = None,\n",
    "    save_model: bool = True\n",
    "    ) -> None:\n",
    "    \n",
    "    if Task.current_task() is not None:\n",
    "        Task.current_task().close()\n",
    "    task = Task.init(project_name=\"gpt2-pretrain\", task_name=f\"Experiment Run {exp_name}\")\n",
    "\n",
    "    params={\n",
    "        \"batch_size\": batch_size,\n",
    "        \"num_epochs\": 1,\n",
    "        \"seq_len\": seq_len,\n",
    "        \"optimizer\": \"AdamW\",\n",
    "        \"scheduler\": scheduler,\n",
    "        \"gradient_accumulation\": gradient_accumulation,\n",
    "        \"learning_rate\": 5e-4,\n",
    "        \"num_warmup_steps\": num_warmup_steps,\n",
    "        \"use_torch_attention\": use_torch_attention,\n",
    "        \"use_grad_checkpoint\": use_grad_checkpoint\n",
    "\n",
    "    }\n",
    "\n",
    "    task.connect(params)\n",
    "\n",
    "    model = GPT.from_pretrained(\"gpt2\", use_torch_attention=params[\"use_torch_attention\"])\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    batch_size = params[\"batch_size\"]\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "    learning_rate = params[\"learning_rate\"]\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    num_epochs = params[\"num_epochs\"]\n",
    "    num_training_steps = len(train_loader) * num_epochs\n",
    "    num_warmup_steps = params[\"num_warmup_steps\"]\n",
    "\n",
    "    if params[\"scheduler\"] == \"linear_with_warmup\":\n",
    "        # Вариант 1: Linear schedule (линейное уменьшение learning rate)\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=num_warmup_steps,\n",
    "            num_training_steps=num_training_steps\n",
    "        )\n",
    "    elif params[\"scheduler\"] == \"cosine_with_warmup\":\n",
    "        # Вариант 2: Cosine schedule (косинусное изменение learning rate)\n",
    "        # Cosine schedule уменьшает learning rate по косинусоиде, что часто дает лучшие результаты\n",
    "        # Особенно полезно для длительного обучения\n",
    "        scheduler = get_cosine_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=num_warmup_steps,\n",
    "            num_training_steps=num_training_steps,\n",
    "            num_cycles=0.5  # Количество полных циклов косинуса (0.5 = половина цикла)\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"scheduler is not configured\")\n",
    "\n",
    "    print(\"Начинаем обучение...\\n\")\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_perplexities = []\n",
    "    gradient_accumulation_steps = params[\"gradient_accumulation\"]\n",
    "\n",
    "    for epoch in range(params[\"num_epochs\"]):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        \n",
    "        for batch_idx, batch in enumerate(progress_bar):\n",
    "            if max_batches is not None and batch_idx > max_batches:\n",
    "                break\n",
    "            inputs, targets = batch\n",
    "            preds, loss = model(inputs, targets=targets)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            if (batch_idx + 1) % gradient_accumulation_steps == 0:\n",
    "                # Зачем необходимо клипать градиенты?\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                # Сделайте шаги оптимизатора и шедулера\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            \n",
    "            current_lr = scheduler.get_last_lr()[0]\n",
    "            progress_bar.set_postfix({'loss': loss.item()})\n",
    "            \n",
    "            # Log batch-level metrics to ClearML\n",
    "            if 'task' in globals():\n",
    "                global_step = epoch * len(train_loader) + batch_idx\n",
    "                task.logger.report_scalar(\n",
    "                    title=\"Training/Batch\",\n",
    "                    series=\"Loss\",\n",
    "                    value=loss.item(),\n",
    "                    iteration=global_step\n",
    "                )\n",
    "                task.logger.report_scalar(\n",
    "                    title=\"Training/Batch\",\n",
    "                    series=\"Learning Rate\",\n",
    "                    value=current_lr,\n",
    "                    iteration=global_step\n",
    "                )\n",
    "        \n",
    "        avg_train_loss = epoch_loss / num_batches\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        # Валидация\n",
    "        val_loss, val_ppl = validate(model, val_loader, device)\n",
    "        val_losses.append(val_loss)\n",
    "        val_perplexities.append(val_ppl)\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}:\")\n",
    "        print(f\"  Train Loss: {avg_train_loss:.4f}\")\n",
    "        print(f\"  Val Loss: {val_loss:.4f}\")\n",
    "        print(f\"  Val Perplexity: {val_ppl:.2f}\\n\")\n",
    "        \n",
    "        # Log epoch-level metrics to ClearML\n",
    "        if 'task' in globals():\n",
    "            task.logger.report_scalar(\n",
    "                title=\"Training/Epoch\",\n",
    "                series=\"Loss\",\n",
    "                value=avg_train_loss,\n",
    "                iteration=epoch + 1\n",
    "            )\n",
    "            task.logger.report_scalar(\n",
    "                title=\"Validation/Epoch\",\n",
    "                series=\"Loss\",\n",
    "                value=val_loss,\n",
    "                iteration=epoch + 1\n",
    "            )\n",
    "            task.logger.report_scalar(\n",
    "                title=\"Validation/Epoch\",\n",
    "                series=\"Perplexity\",\n",
    "                value=val_ppl,\n",
    "                iteration=epoch + 1\n",
    "            )\n",
    "            task.logger.report_scalar(\n",
    "                title=\"Training/Epoch\",\n",
    "                series=\"Learning Rate\",\n",
    "                value=scheduler.get_last_lr()[0],\n",
    "                iteration=epoch + 1\n",
    "            )\n",
    "\n",
    "    print(\"Обучение завершено!\")\n",
    "\n",
    "    # Тестируем генерацию\n",
    "    test_prompts = [\n",
    "        \"The future of\",\n",
    "        \"Machine learning\",\n",
    "        \"Ох уж эти мне сказочники! \",\n",
    "        \"Петербург выглядел \",\n",
    "        \"Где находится Москва? \"\n",
    "    ]\n",
    "\n",
    "    print(\"Генерация текста обученной моделью:\\n\")\n",
    "    for prompt in test_prompts:\n",
    "        generated = generate_text(prompt, max_length=60, top_k=20)\n",
    "        print(f\"Prompt: '{prompt}'\")\n",
    "        print(f\"Generated: {generated}\\n\")\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "\n",
    "    if save_model:\n",
    "        save_path = \"./gpt2_pretrained\"\n",
    "        torch.save(model.state_dict(), save_path+f\"/model_{exp_name}.pth\")\n",
    "        print(f\"Модель сохранена в {save_path} с именем model_{exp_name}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd11f1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 1024\n",
    "run_exp(0, task, use_grad_checkpoint=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5677213",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 8\n",
    "for gradient_accumulation in [4, 8, 16]:\n",
    "    for num_warmup_steps in [10, 20]:\n",
    "        for scheduler in [\"cosine_with_warmup\", \"linear_with_warmup\"]:\n",
    "            run_exp(i, task, gradient_accumulation=gradient_accumulation, num_warmup_steps=num_warmup_steps, scheduler=scheduler)\n",
    "            i += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629f8d13",
   "metadata": {},
   "source": [
    "Вопросы:\n",
    ".\n",
    "- Почему мы используем сдвиг на 1 токен между inputs и labels?\n",
    "- Что означает perplexity и почему она важна для языковых моделей?\n",
    "- Зачем нужен gradient clipping?\n",
    "- Как learning rate scheduler помогает в обучении?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f515b9",
   "metadata": {},
   "source": [
    "Запустим финальный прогон с наилучшими, по нашему мнению, параметрами:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa314ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_exp(\"Final_pretrain\", task, gradient_accumulation=4, num_warmup_steps=20, scheduler=\"cosine_with_warmup\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f1ee36",
   "metadata": {},
   "source": [
    "Для оценки финальной модели вычисли м perplexity. Вообще наилучшая практика - оценка на одном или нескольких downstream датасетах, но это достаточно затратно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95df3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Вычисляем финальную perplexity\n",
    "final_val_loss, final_val_ppl = validate(model, val_loader, device)\n",
    "print(f\"Финальная валидационная perplexity: {final_val_ppl:.2f}\")\n",
    "\n",
    "# Для сравнения: perplexity предобученной модели на наших данных\n",
    "model.eval()\n",
    "pretrained_loss = 0\n",
    "pretrained_tokens = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        batch = batch.to(device)\n",
    "        inputs = batch[:, :-1]\n",
    "        labels = batch[:, 1:]\n",
    "        \n",
    "        outputs = model(inputs, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        mask = (labels != tokenizer.pad_token_id).float()\n",
    "        num_tokens = mask.sum().item()\n",
    "        \n",
    "        pretrained_loss += loss.item() * num_tokens\n",
    "        pretrained_tokens += num_tokens\n",
    "\n",
    "pretrained_avg_loss = pretrained_loss / pretrained_tokens if pretrained_tokens > 0 else 0\n",
    "pretrained_ppl = calculate_perplexity(pretrained_avg_loss)\n",
    "\n",
    "print(f\"Perplexity предобученной GPT-2 на наших данных: {pretrained_ppl:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d554c4af",
   "metadata": {},
   "source": [
    "Вопросы:\n",
    "- Почему наша маленькая модель может генерировать менее качественный текст, чем предобученная?\n",
    "- Как можно улучшить качество модели на downstream задачах без увеличения размера?\n",
    "- Что такое perplexity и как интерпретировать её значения?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25aab39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сохранение модели\n",
    "save_path = \"./gpt2_pretrained\"\n",
    "model.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)\n",
    "print(f\"Модель сохранена в {save_path}\")\n",
    "\n",
    "# Загрузка модели (пример)\n",
    "# loaded_model = GPT2LMHeadModel.from_pretrained(save_path)\n",
    "# loaded_tokenizer = GPT2Tokenizer.from_pretrained(save_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0564f3",
   "metadata": {},
   "source": [
    "### Возможные улучшения:\n",
    "1. **Gradient checkpointing**: для экономии памяти\n",
    "2. **Mixed Precision Training**: использование FP16 для ускорения обучения\n",
    "3. **Больше данных**: использование больших корпусов текста и большего числа эпох\n",
    "5. **Fine-tuning**: дообучение на специфических данных\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7c24b0",
   "metadata": {},
   "source": [
    "Источники:\n",
    "1) лекция Андрея Карпатого [Let's reproduce GPT-2 (124M)](https://www.youtube.com/watch?v=l8pRSuU81PU)\n",
    "2) [Smol training playbook](https://huggingface.co/spaces/HuggingFaceTB/smol-training-playbook)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93861c8c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
